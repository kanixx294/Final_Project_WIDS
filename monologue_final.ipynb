{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98596d9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer\n",
    "\n",
    "lines = {}\n",
    "with open(\"movie_lines.txt\", encoding=\"ISO-8859-1\") as f:\n",
    "    for line in f:\n",
    "        p = line.strip().split(\" +++$+++ \")\n",
    "        if len(p) == 5: lines[p[0]] = p[4]\n",
    "\n",
    "convs = []\n",
    "with open(\"movie_conversations.txt\", encoding=\"ISO-8859-1\") as f:\n",
    "    for line in f:\n",
    "        p = line.strip().split(\" +++$+++ \")\n",
    "        if len(p) == 4: convs.append(eval(p[3]))\n",
    "\n",
    "pairs = []\n",
    "for c in convs:\n",
    "    for i in range(len(c) - 1):\n",
    "        if c[i] in lines and c[i+1] in lines:\n",
    "            q, a = lines[c[i]].strip(), lines[c[i+1]].strip()\n",
    "            if 3 < len(q.split()) < 25 and 3 < len(a.split()) < 25:\n",
    "                pairs.append({\"input\": q, \"output\": a})\n",
    "\n",
    "model_id = \"microsoft/DialoGPT-medium\"\n",
    "tk = AutoTokenizer.from_pretrained(model_id)\n",
    "tk.pad_token = tk.eos_token\n",
    "data = Dataset.from_list(random.sample(pairs, 5000))\n",
    "\n",
    "def tokenize(item):\n",
    "    txt = item[\"input\"] + tk.eos_token + item[\"output\"] + tk.eos_token\n",
    "    res = tk(txt, truncation=True, padding=\"max_length\", max_length=64)\n",
    "    res[\"labels\"] = res[\"input_ids\"].copy()\n",
    "    return res\n",
    "\n",
    "tokenized_data = data.map(tokenize)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    per_device_train_batch_size=1,\n",
    "    gradient_accumulation_steps=8,\n",
    "    num_train_epochs=2,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "trainer = Trainer(model=model, args=args, train_dataset=tokenized_data, tokenizer=tk)\n",
    "trainer.train()\n",
    "\n",
    "trainer.save_model(\"./final_model\")\n",
    "tk.save_pretrained(\"./final_model\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
